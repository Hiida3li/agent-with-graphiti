{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MockLogger:\n",
    "    def debug(self, message):\n",
    "        print(f\"DEBUG: {message}\")\n",
    "\n",
    "logger = MockLogger()\n",
    "\n",
    "class Response:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "response = Response(\"This is a long response from the API that contains lots of text and goes beyond 200 characters to show how the truncation works in the debug message when logging the response content.\")\n",
    "\n",
    "print(\"Testing the code:\")\n",
    "if hasattr(response, 'text') and response.text:\n",
    "    logger.debug(f\"Gemini response received: {response.text[:200]}...\")\n",
    "    result = response.text\n",
    "    print(f\"Returned: {result[:50]}...\")\n"
   ],
   "id": "45939d2d9e9a8cc5"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-10T14:09:33.631505Z",
     "start_time": "2025-07-10T14:09:33.330791Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahmoud/agent-with-graphiti/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T14:09:39.562810Z",
     "start_time": "2025-07-10T14:09:39.557504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LLMProvider(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "class GeminiProvider(LLMProvider):\n",
    "    def __init__(self, model_name: str = \"gemini-2.5-flash\"):\n",
    "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        logger.debug(f\"Generating response for prompt length: {len(prompt)}\")\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.debug(f\"Gemini API call attempt {attempt + 1}\")\n",
    "                response = self.model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config=genai.types.GenerationConfig(\n",
    "                        temperature=0,\n",
    "                        max_output_tokens=500,\n",
    "                        top_p=0,\n",
    "                        top_k=40\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                if hasattr(response, 'text') and response.text:\n",
    "                    logger.debug(f\"Gemini response received: {response.text[:200]}...\")\n",
    "                    return response.text\n",
    "\n",
    "                logger.warning(f\"Empty response from Gemini (attempt {attempt + 1})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Gemini API error (attempt {attempt + 1}): {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    fallback_response = '{\"thought\": \"Technical difficulties\", \"answer\": \"I\\'m experiencing technical issues. Please try again.\"}'\n",
    "                    logger.debug(f\"Returning fallback response: {fallback_response}\")\n",
    "                    return fallback_response\n",
    "\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "\n",
    "        fallback_response = '{\"thought\": \"Max retries exceeded\", \"answer\": \"Connection issues. Please try again later.\"}'\n",
    "        logger.debug(f\"Max retries exceeded, returning: {fallback_response}\")\n",
    "        return fallback_response"
   ],
   "id": "f7044dab1250df46",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T14:12:31.313682Z",
     "start_time": "2025-07-10T14:12:27.521792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simple test template for ReAct agent\n",
    "def create_test_prompt(query: str, history: str = \"\") -> str:\n",
    "    return f\"\"\"You are an expert Customer Service Agent. Analyze the user's query to understand their intent and plan the appropriate response with the provided tools.\n",
    "Query: {query}\n",
    "History: {history}\n",
    "Available tools: SEARCH_PRODUCTS, PLACE_ORDER, GET_ORDER_HISTORY\n",
    "\n",
    "For SEARCH_PRODUCTS tool, use separate attributes:\n",
    "- \"input\": user input\n",
    "- \"color\": color if specified\n",
    "- \"model\": model/variant if specified\n",
    "Respond in JSON format:\n",
    "{{\"thought\": \"reasoning\", \"action\": {{\"name\": \"SEARCH_PRODUCTS\", \"input\": \"iPhone\", \"color\": \"red\", \"model\": \"15 Pro\"}}}}\n",
    "OR\n",
    "{{\"thought\": \"reasoning\", \"answer\": \"final answer\"}}\n",
    "\"\"\"\n",
    "\n",
    "llm_provider = GeminiProvider()\n",
    "\n",
    "prompt1 = create_test_prompt(\"I want to bur latest iPhone\")\n",
    "print(f\"PROMPT:\\n{prompt1}\")\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "response1 = llm_provider.generate(prompt1)\n",
    "print(f\"JSON RESPONSE:\\n{response1}\")\n",
    "\n"
   ],
   "id": "444ebb7b2dbdaf61",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 18:12:27,523 - DEBUG - Generating response for prompt length: 601\n",
      "2025-07-10 18:12:27,524 - DEBUG - Gemini API call attempt 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "You are an expert Customer Service Agent. Analyze the user's query to understand their intent and plan the appropriate response with the provided tools.\n",
      "Query: I want to bur latest iPhone\n",
      "History: \n",
      "Available tools: SEARCH_PRODUCTS, PLACE_ORDER, GET_ORDER_HISTORY\n",
      "\n",
      "For SEARCH_PRODUCTS tool, use separate attributes:\n",
      "- \"input\": user input\n",
      "- \"color\": color if specified\n",
      "- \"model\": model/variant if specified\n",
      "Respond in JSON format:\n",
      "{\"thought\": \"reasoning\", \"action\": {\"name\": \"SEARCH_PRODUCTS\", \"input\": \"iPhone\", \"color\": \"red\", \"model\": \"15 Pro\"}}\n",
      "OR\n",
      "{\"thought\": \"reasoning\", \"answer\": \"final answer\"}\n",
      "\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 18:12:31,309 - DEBUG - Gemini response received: ```json\n",
      "{\n",
      "  \"thought\": \"The user wants to 'buy' (likely a typo for 'bur') the 'latest iPhone'. To fulfill this request, I need to search for iPhones. The `SEARCH_PRODUCTS` tool is appropriate for this...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON RESPONSE:\n",
      "```json\n",
      "{\n",
      "  \"thought\": \"The user wants to 'buy' (likely a typo for 'bur') the 'latest iPhone'. To fulfill this request, I need to search for iPhones. The `SEARCH_PRODUCTS` tool is appropriate for this. Since the user specified 'latest' but no specific model number or color, I will search for 'iPhone' generally. The system or a subsequent step would then identify the 'latest' model from the search results.\",\n",
      "  \"action\": {\n",
      "    \"name\": \"SEARCH_PRODUCTS\",\n",
      "    \"input\": \"iPhone\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T14:13:34.595619Z",
     "start_time": "2025-07-10T14:13:32.844768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# history = \"user: I want to search for iPhone\\nassistant: \" + response1\n",
    "prompt2 = create_test_prompt(\"15 Pro in red color\")\n",
    "\n",
    "response2 = llm_provider.generate(prompt2)\n",
    "print(f\"JSON RESPONSE:\\n{response2}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 50)\n",
    "# print(\"=\" * 50)\n",
    "# print(f\"Response 1: {response1}\")\n",
    "# print(f\"Response 2: {response2}\")"
   ],
   "id": "a424744b3e168a7f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 18:13:32,845 - DEBUG - Generating response for prompt length: 593\n",
      "2025-07-10 18:13:32,846 - DEBUG - Gemini API call attempt 1\n",
      "2025-07-10 18:13:34,591 - DEBUG - Gemini response received: ```json\n",
      "{\n",
      "  \"thought\": \"The user is looking for a specific product, '15 Pro' with a 'red' color. The `SEARCH_PRODUCTS` tool is appropriate for this. I can extract '15 Pro' as the model and 'red' as th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON RESPONSE:\n",
      "```json\n",
      "{\n",
      "  \"thought\": \"The user is looking for a specific product, '15 Pro' with a 'red' color. The `SEARCH_PRODUCTS` tool is appropriate for this. I can extract '15 Pro' as the model and 'red' as the color. Since '15 Pro' is commonly associated with iPhone, I will use 'iPhone' as the general input.\",\n",
      "  \"action\": {\n",
      "    \"name\": \"SEARCH_PRODUCTS\",\n",
      "    \"input\": \"iPhone\",\n",
      "    \"color\": \"red\",\n",
      "    \"model\": \"15 Pro\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
