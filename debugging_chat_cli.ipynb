{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T06:25:58.792268Z",
     "start_time": "2025-07-23T06:25:57.752894Z"
    }
   },
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai import errors\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "from string import Template\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if GOOGLE_API_KEY is None:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables. Please set it in your .env file.\")\n",
    "\n",
    "search_products_function = types.FunctionDeclaration(\n",
    "    name=\"search_products\",\n",
    "    description=\"Searches the product catalog for items based on a text query and optional filters.\",\n",
    "    parameters_json_schema={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"A search query for products.\"\n",
    "            },\n",
    "            \"filters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"Optional filters like color.\",\n",
    "                \"properties\": {\n",
    "                    \"color\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "search_faqs_function = types.FunctionDeclaration(\n",
    "    name=\"search_faqs\",\n",
    "    description=\"Searches a knowledge base of Frequently Asked Questions (FAQs) based on a user's query. Returns a list of relevant FAQs including their questions, answers, and categories.\",\n",
    "    parameters_json_schema={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's question or search query to find relevant FAQs. For example: 'How long does shipping take?' or 'return policy'.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"text\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "respond_to_user_function = types.FunctionDeclaration(\n",
    "    name=\"respond_to_user\",\n",
    "    description=\"Use this tool to send a response directly to the user. This tool also handles the formatting and routing of the message, and allows the AI to incorporate all available information to send an informative response that includes thoughts, function calls and instructions.\",\n",
    "    parameters_json_schema={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The response to be sent to the user\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"content\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "TOOLS = [\n",
    "    types.Tool(function_declarations=[\n",
    "        search_products_function,\n",
    "        search_faqs_function,\n",
    "        respond_to_user_function\n",
    "    ])\n",
    "]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T06:27:24.586783Z",
     "start_time": "2025-07-23T06:27:24.530714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LLMProvider(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str):\n",
    "        pass\n",
    "\n",
    "\n",
    "class GeminiProvider(LLMProvider):\n",
    "    def __init__(self, model_name: str = \"gemini-2.5-flash\"):\n",
    "        try:\n",
    "            api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "            if not api_key:\n",
    "                raise ValueError(\"GOOGLE_API_KEY or GEMINI_API_KEY environment variable required\")\n",
    "\n",
    "\n",
    "            self.client = genai.Client(api_key=api_key)\n",
    "            self.model_name = model_name\n",
    "\n",
    "            logger.debug(f\"GeminiProvider initialized successfully:\")\n",
    "            logger.debug(f\"  - Model: {model_name}\")\n",
    "            logger.debug(f\"  - Client type: {type(self.client)}\")\n",
    "            logger.debug(\n",
    "                f\"  - Tools configured: {len(TOOLS)} tool groups with functions: {[func.name for tool in TOOLS for func in tool.function_declarations]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize GeminiProvider: {e}\")\n",
    "            logger.error(f\"Available environment variables:\")\n",
    "            logger.error(f\"  - GOOGLE_API_KEY: {'***SET***' if os.getenv('GOOGLE_API_KEY') else 'NOT SET'}\")\n",
    "            logger.error(f\"  - GEMINI_API_KEY: {'***SET***' if os.getenv('GEMINI_API_KEY') else 'NOT SET'}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str):\n",
    "        logger.debug(f\"Generating response for prompt length: {len(prompt)}\")\n",
    "        logger.debug(f\"Prompt preview (first 200 chars): {prompt[:200]}...\")\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.debug(f\"Gemini API call attempt {attempt + 1}\")\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model_name,\n",
    "                    contents=prompt,\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        tools=TOOLS,\n",
    "                        tool_config=types.ToolConfig(\n",
    "                            function_calling_config=types.FunctionCallingConfig(mode='ANY')\n",
    "                        ),\n",
    "                        temperature=0,\n",
    "                        max_output_tokens=10000,\n",
    "                        top_p=0.95\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                if response and response.candidates:\n",
    "                    logger.debug(\"Successfully received a valid response from Gemini.\")\n",
    "                    logger.debug(f\"Response has {len(response.candidates)} candidates\")\n",
    "                    return response\n",
    "\n",
    "                logger.warning(f\"Empty response from Gemini on attempt {attempt + 1}.\")\n",
    "\n",
    "            except errors.APIError as e:\n",
    "                logger.error(f\"Gemini API error on attempt {attempt + 1}: {e.code} - {e.message}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Gemini API error on attempt {attempt + 1}: {e}\")\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "\n",
    "        logger.error(\"All retry attempts to reach Gemini failed.\")\n",
    "        return None\n"
   ],
   "id": "6fc024356c938b97",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_function_args(function_call_args):\n",
    "    \"\"\"Extract function call arguments in JSON-serializable format for new SDK\"\"\"\n",
    "    try:\n",
    "\n",
    "        if isinstance(function_call_args, dict):\n",
    "\n",
    "            logger.debug(\"Function args in direct dictionary format\")\n",
    "            return function_call_args\n",
    "\n",
    "\n",
    "        if hasattr(function_call_args, 'dict'):\n",
    "            logger.debug(\"Function args in Pydantic model format\")\n",
    "            return function_call_args.dict()\n",
    "\n",
    "        # Handle if it's an object with direct attribute access\n",
    "        if hasattr(function_call_args, '__dict__'):\n",
    "            logger.debug(\"Function args in object format\")\n",
    "            return function_call_args.__dict__\n",
    "\n",
    "        # Legacy format handling (keep for backward compatibility)\n",
    "        if hasattr(function_call_args, 'items'):\n",
    "            logger.debug(\"Function args in legacy format - converting\")\n",
    "            args_dict = {}\n",
    "            for key, value in function_call_args.items():\n",
    "                if hasattr(value, 'string_value'):\n",
    "                    args_dict[key] = value.string_value\n",
    "                elif hasattr(value, 'struct_value'):\n",
    "                    # Handle nested structures like filters\n",
    "                    nested_dict = {}\n",
    "                    for nested_key, nested_value in value.struct_value.fields.items():\n",
    "                        if hasattr(nested_value, 'string_value'):\n",
    "                            nested_dict[nested_key] = nested_value.string_value\n",
    "                        else:\n",
    "                            nested_dict[nested_key] = str(nested_value)\n",
    "                    args_dict[key] = nested_dict\n",
    "                elif hasattr(value, 'number_value'):\n",
    "                    args_dict[key] = value.number_value\n",
    "                elif hasattr(value, 'bool_value'):\n",
    "                    args_dict[key] = value.bool_value\n",
    "                else:\n",
    "                    # Fallback to string conversion\n",
    "                    args_dict[key] = str(value)\n",
    "            return args_dict\n",
    "\n",
    "        # Final fallback - try to convert to string and parse as JSON\n",
    "        logger.warning(f\"Unexpected function args format: {type(function_call_args)}\")\n",
    "        return json.loads(str(function_call_args)) if str(function_call_args) else {}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not extract function args: {e}\")\n",
    "        logger.warning(f\"Args type: {type(function_call_args)}\")\n",
    "        logger.warning(f\"Args content: {function_call_args}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_final_response(context: Dict[str, Any], prompt_template: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate final LLM response - ONLY returns function call info (Mode 2 only)\"\"\"\n",
    "    logger.debug(\"Starting final response generation\")\n",
    "    from string import Template\n",
    "\n",
    "    llm_provider = GeminiProvider()\n",
    "\n",
    "    function_results = \"\"\n",
    "    latest_interaction = context.get(\"interactions\", [])[-1] if context.get(\"interactions\") else None\n",
    "\n",
    "    logger.debug(f\"Context has {len(context.get('interactions', []))} interactions\")\n",
    "    logger.debug(f\"Latest interaction: {latest_interaction.get('interaction_id') if latest_interaction else 'None'}\")\n",
    "\n",
    "    if latest_interaction:\n",
    "        logger.debug(f\"Processing {len(latest_interaction.get('function_executions', []))} function executions\")\n",
    "        for execution in latest_interaction.get(\"function_executions\", []):\n",
    "            if execution.get(\"execution_status\") == \"completed\":\n",
    "                func_name = execution.get(\"function_name\", \"\")\n",
    "                result = execution.get(\"execution_result\", {})\n",
    "                logger.debug(f\"Including result from function: {func_name}\")\n",
    "\n",
    "                if result is not None:\n",
    "                    function_results += f\"\\nFunction: {func_name}\\n\"\n",
    "                    # Use the formatted_summary if the service provided it, otherwise use raw result\n",
    "                    if \"formatted_summary\" in result:\n",
    "                        function_results += result[\"formatted_summary\"]\n",
    "                    else:\n",
    "                        function_results += f\"Result: {json.dumps(result, indent=2)}\\n\"\n",
    "\n",
    "    logger.debug(f\"Function results length: {len(function_results)} chars\")\n",
    "\n",
    "    template = Template(prompt_template)\n",
    "    final_prompt = template.safe_substitute(\n",
    "        query=context.get(\"query\", \"\"),\n",
    "        interactions=json.dumps(context.get(\"interactions\", []), indent=2),\n",
    "        history=\"\\n\".join(context.get(\"history\", [])),\n",
    "        function_results=function_results\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"Generating final response with prompt length: {len(final_prompt)}\")\n",
    "    final_response_object = llm_provider.generate(final_prompt)\n",
    "\n",
    "    if final_response_object and final_response_object.candidates:\n",
    "        logger.debug(\"Processing LLM final response candidates\")\n",
    "        part = final_response_object.candidates[0].content.parts[0]\n",
    "        logger.debug(f\"Response part type: {type(part)}\")\n",
    "        logger.debug(f\"Part has function_call: {hasattr(part, 'function_call') and part.function_call}\")\n",
    "        logger.debug(f\"Part has text: {hasattr(part, 'text') and bool(part.text)}\")\n",
    "\n",
    "        if part.function_call:\n",
    "            logger.debug(f\"Function call detected in final response: {part.function_call.name}\")\n",
    "\n",
    "            args = extract_function_args(part.function_call.args)\n",
    "\n",
    "            return {\n",
    "                \"type\": \"function_call\",\n",
    "                \"function\": {\n",
    "                    \"name\": part.function_call.name,\n",
    "                    \"args\": args,\n",
    "                    \"id\": str(uuid.uuid4())\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "\n",
    "            logger.warning(\"LLM did not call any function - this should not happen in Mode 2 only\")\n",
    "            return {\n",
    "                \"type\": \"function_call\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"final_response\",\n",
    "                    \"args\": {\"content\": \"I'm sorry, I couldn't generate a proper response.\"},\n",
    "                    \"id\": str(uuid.uuid4())\n",
    "                }\n",
    "            }\n",
    "    else:\n",
    "        logger.warning(\"LLM final response was empty or failed\")\n",
    "        return {\n",
    "            \"type\": \"function_call\",\n",
    "            \"function\": {\n",
    "                \"name\": \"final_response\",\n",
    "                \"args\": {\"content\": \"I'm sorry, I couldn't generate a final response.\"},\n",
    "                \"id\": str(uuid.uuid4())\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def process_message(msg: Dict[str, Any], llm_provider: LLMProvider) -> List[Dict[str, Any]]:\n",
    "    logger.debug(\"=== Starting message processing ===\")\n",
    "\n",
    "    header = msg.get(\"header\", {})\n",
    "    payload = msg.get(\"payload\", {})\n",
    "    agent = payload.get(\"agent\", {})\n",
    "    context = agent.get(\"context\", {})\n",
    "    prompt_template = agent.get(\"prompt\", \"\")\n",
    "\n",
    "    logger.debug(f\"Message header ID: {header.get('id', 'No ID')}\")\n",
    "    logger.debug(f\"Context query: {context.get('query', 'No query')}\")\n",
    "    logger.debug(f\"Context keys: {list(context.keys())}\")\n",
    "    logger.debug(f\"Context history length: {len(context.get('history', []))}\")\n",
    "    logger.debug(f\"Context interactions count: {len(context.get('interactions', []))}\")\n",
    "    logger.debug(f\"Prompt template length: {len(prompt_template)} chars\")\n",
    "\n",
    "    template = Template(prompt_template)\n",
    "\n",
    "    template_context = context.copy()\n",
    "    if 'query' in template_context and 'user_query' not in template_context:\n",
    "        template_context['user_query'] = template_context['query']\n",
    "\n",
    "    if len(template_context.get('history', [])) > 0:\n",
    "        logger.warning(f\"Clearing {len(template_context['history'])} contaminated history items for testing\")\n",
    "        template_context['history'] = []\n",
    "\n",
    "    prompt = template.safe_substitute(**template_context)\n",
    "    logger.debug(f\"Template context keys: {list(template_context.keys())}\")\n",
    "\n",
    "    if 'history' in template_context:\n",
    "        logger.debug(f\"History items: {template_context['history']}\")\n",
    "\n",
    "    logger.debug(f\"Substituted prompt length: {len(prompt)} chars\")\n",
    "\n",
    "    logger.debug(\"Calling LLM provider...\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FULL PROMPT BEING SENT TO GEMINI:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"END PROMPT\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    llm_response_object = llm_provider.generate(prompt)\n",
    "\n",
    "    function_calls = []\n",
    "    direct_response = \"\"\n",
    "    llm_reasoning = \"\"\n",
    "\n",
    "    if llm_response_object and llm_response_object.candidates:\n",
    "        logger.debug(\"Processing LLM response candidates\")\n",
    "        part = llm_response_object.candidates[0].content.parts[0]\n",
    "        logger.debug(f\"Response part type: {type(part)}\")\n",
    "        logger.debug(f\"Part has function_call: {hasattr(part, 'function_call') and part.function_call}\")\n",
    "        logger.debug(f\"Part has text: {hasattr(part, 'text') and bool(part.text)}\")\n",
    "\n",
    "        if part.function_call:\n",
    "            logger.debug(f\"Function call detected: {part.function_call.name}\")\n",
    "            args = extract_function_args(part.function_call.args)\n",
    "            llm_reasoning = f\"Decided to call function: {part.function_call.name}\"\n",
    "            logger.debug(f\"LLM reasoning: {llm_reasoning}\")\n",
    "            logger.debug(f\"Function args: {args}\")\n",
    "            function_calls.append({\n",
    "                \"name\": part.function_call.name,\n",
    "                \"args\": args\n",
    "            })\n",
    "\n",
    "\n",
    "        else:\n",
    "            logger.debug(\"Direct response from LLM\")\n",
    "            direct_response = llm_response_object.text\n",
    "            llm_reasoning = \"Provided a direct answer.\"\n",
    "            logger.debug(f\"Direct response length: {len(direct_response)} chars\")\n",
    "            logger.debug(f\"Direct response content: '{direct_response}'\")\n",
    "    else:\n",
    "        logger.warning(\"LLM response was empty or failed\")\n",
    "        direct_response = \"I'm having trouble connecting. Please try again.\"\n",
    "        llm_reasoning = \"LLM response was empty or failed.\"\n",
    "\n",
    "    if \"interactions\" not in context:\n",
    "        context[\"interactions\"] = []\n",
    "        logger.debug(\"Initialized empty interactions list\")\n",
    "    if \"history\" not in context:\n",
    "        context[\"history\"] = []\n",
    "        logger.debug(\"Initialized empty history list\")\n",
    "\n",
    "    logger.debug(\n",
    "        f\"Current context has {len(context['interactions'])} interactions and {len(context['history'])} history items\")\n",
    "\n",
    "    response_entry = {\n",
    "        \"interaction_id\": str(uuid.uuid4()),\n",
    "        \"timestamp\": time.time(),\n",
    "        \"user_query\": context.get(\"query\", \"\"),\n",
    "        \"llm_reasoning\": llm_reasoning,\n",
    "        \"function_executions\": []\n",
    "    }\n",
    "    logger.debug(f\"Created response entry with ID: {response_entry['interaction_id']}\")\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    if function_calls:\n",
    "        logger.debug(f\"Processing {len(function_calls)} function calls\")\n",
    "        first_func_call = function_calls[0]\n",
    "        for i, func_call in enumerate(function_calls):\n",
    "            execution_id = str(uuid.uuid4())\n",
    "            logger.debug(f\"Creating execution {i + 1}/{len(function_calls)} with ID: {execution_id}\")\n",
    "\n",
    "            function_execution = {\n",
    "                \"execution_id\": execution_id,\n",
    "                \"function_name\": func_call.get(\"name\"),\n",
    "                \"parameters\": func_call.get(\"args\", {}),\n",
    "                \"execution_status\": \"pending\" if i == 0 else \"queued\",\n",
    "            }\n",
    "            response_entry[\"function_executions\"].append(function_execution)\n",
    "            if i == 0:\n",
    "                first_func_call[\"id\"] = execution_id\n",
    "\n",
    "            args_str = \", \".join([f\"{k}={v}\" for k, v in func_call.get(\"args\", {}).items()])\n",
    "            tool_call_log = f\"Tool Call: {func_call.get('name')}({args_str})\"\n",
    "            context[\"history\"].append(tool_call_log)\n",
    "            logger.debug(f\"Added to history: {tool_call_log}\")\n",
    "\n",
    "        message = {\n",
    "            \"header\": header,\n",
    "            \"payload\": {\n",
    "                \"agent\": {\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": prompt_template,\n",
    "                    \"current_function_execution\": first_func_call,\n",
    "                    \"remaining_function_calls\": function_calls[1:] if len(function_calls) > 1 else []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        messages.append(message)\n",
    "        response_entry[\"response_type\"] = \"function_assisted\"\n",
    "        context[\"interactions\"].append(response_entry)\n",
    "        logger.debug(\"Created function-assisted response message\")\n",
    "\n",
    "    else:\n",
    "        logger.debug(\"Creating direct knowledge response\")\n",
    "        response_entry[\"response_type\"] = \"direct_knowledge\"\n",
    "        response_entry[\"direct_llm_response\"] = direct_response\n",
    "        response_entry[\"response\"] = direct_response\n",
    "        context[\"interactions\"].append(response_entry)\n",
    "\n",
    "        messages.append({\n",
    "            \"header\": header,\n",
    "            \"payload\": {\n",
    "                \"agent\": {\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": prompt_template,\n",
    "                    \"response_type\": \"direct_knowledge\",\n",
    "                    \"response\": direct_response\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    logger.debug(f\"=== Message processing complete. Returning {len(messages)} messages ===\")\n",
    "    return messages\n",
    "\n",
    "\n",
    "def process_function_response(msg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process function call responses and update the structured hierarchy\"\"\"\n",
    "    logger.debug(\"=== Starting function response processing ===\")\n",
    "    import time\n",
    "\n",
    "    agent = msg.get(\"payload\", {}).get(\"agent\", {})\n",
    "    function_call = agent.get(\"function_call\", {})\n",
    "\n",
    "    if \"response\" in function_call:\n",
    "        func_call_id = function_call.get(\"id\")\n",
    "        function_name = function_call.get(\"name\")\n",
    "        logger.info(f\"Function {function_name} (ID: {func_call_id}) completed successfully\")\n",
    "\n",
    "        # Get context and its interactions\n",
    "        context = agent.get(\"context\", {})\n",
    "        interactions = context.get(\"interactions\", [])\n",
    "\n",
    "        logger.debug(f\"Context has {len(interactions)} interactions\")\n",
    "\n",
    "        function_result = function_call[\"response\"]\n",
    "        tool_response_log = f\"Tool Response: {json.dumps(function_result)}\"\n",
    "        context[\"history\"].append(tool_response_log)\n",
    "        logger.debug(\n",
    "            f\"Added tool response to history (result keys: {list(function_result.keys()) if isinstance(function_result, dict) else 'non-dict result'})\")\n",
    "\n",
    "        execution_found = False\n",
    "        for interaction in interactions:\n",
    "            for execution in interaction.get(\"function_executions\", []):\n",
    "                if execution.get(\"execution_id\") == func_call_id:\n",
    "                    logger.debug(f\"Found matching execution in interaction {interaction.get('interaction_id')}\")\n",
    "                    execution[\"execution_status\"] = \"completed\"\n",
    "                    execution[\"completed_at\"] = time.time()\n",
    "                    execution[\"execution_result\"] = function_result\n",
    "                    execution_found = True\n",
    "                    break\n",
    "            if execution_found:\n",
    "                break\n",
    "\n",
    "        if not execution_found:\n",
    "            logger.warning(f\"Could not find execution with ID {func_call_id} to update\")\n",
    "\n",
    "        latest_interaction = interactions[-1] if interactions else None\n",
    "        all_completed = False\n",
    "\n",
    "        if latest_interaction:\n",
    "            executions = latest_interaction.get(\"function_executions\", [])\n",
    "            logger.debug(f\"Latest interaction has {len(executions)} executions\")\n",
    "\n",
    "            completed_count = sum(1 for exec in executions if exec.get(\"execution_status\") == \"completed\")\n",
    "            logger.debug(f\"Completed executions: {completed_count}/{len(executions)}\")\n",
    "\n",
    "            all_completed = all(\n",
    "                exec.get(\"execution_status\") == \"completed\"\n",
    "                for exec in executions\n",
    "            )\n",
    "            logger.debug(f\"All executions completed: {all_completed}\")\n",
    "\n",
    "            if all_completed:\n",
    "                logger.debug(\"All function executions completed, creating summary\")\n",
    "\n",
    "                latest_interaction[\"execution_summary\"] = {\n",
    "                    \"total_functions_executed\": len(executions),\n",
    "                    \"all_successful\": all(exec.get(\"error_details\") is None for exec in executions),\n",
    "                    \"execution_results\": [exec[\"execution_result\"] for exec in executions if\n",
    "                                          exec.get(\"execution_result\")]\n",
    "                }\n",
    "\n",
    "                response_type = latest_interaction.get(\"response_type\")\n",
    "                logger.debug(f\"Response type: {response_type}\")\n",
    "\n",
    "                if response_type == \"function_assisted\":\n",
    "                    logger.info(\"All function executions completed, generating final synthesized response\")\n",
    "\n",
    "                    final_response_result = generate_final_response(context, agent.get(\"prompt\", \"\"))\n",
    "\n",
    "                    # MODE 1 REMOVED: Only function call routing is supported\n",
    "                    if final_response_result.get(\"type\") == \"function_call\":\n",
    "                        logger.info(f\"LLM requested function call: {final_response_result['function']['name']}\")\n",
    "\n",
    "                        # Create message structure for function routing\n",
    "                        return {\n",
    "                            \"header\": msg.get(\"header\", {}),\n",
    "                            \"payload\": {\n",
    "                                \"agent\": {\n",
    "                                    \"context\": context,\n",
    "                                    \"prompt\": agent.get(\"prompt\", \"\"),\n",
    "                                    \"current_function_execution\": final_response_result[\"function\"]\n",
    "                                }\n",
    "                            },\n",
    "                            \"route_to_function\": True  # Signal for routing\n",
    "                        }\n",
    "\n",
    "                else:\n",
    "                    logger.info(\"Direct knowledge query completed, no synthesis needed\")\n",
    "\n",
    "        context[\"interactions\"] = interactions\n",
    "        logger.debug(\"Updated context interactions\")\n",
    "\n",
    "        final_response = latest_interaction.get(\"response\", \"\") if all_completed and latest_interaction else \"\"\n",
    "        logger.debug(f\"Final response length: {len(final_response)} chars\")\n",
    "\n",
    "        final_msg = {\n",
    "            \"header\": msg.get(\"header\", {}),\n",
    "            \"payload\": {\n",
    "                \"agent\": {\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": agent.get(\"prompt\", \"\"),\n",
    "                    \"response\": final_response\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        logger.debug(\"=== Function response processing complete ===\")\n",
    "        return final_msg\n",
    "\n",
    "    logger.debug(\"No response in function call, returning original message\")\n",
    "    return msg\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NOTEBOOK-FRIENDLY INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "class NotebookLLMService:\n",
    "    \"\"\"Jupyter notebook interface for the LLM service\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm_provider = GeminiProvider()\n",
    "        print(\"✅ LLM Service initialized for Jupyter notebook\")\n",
    "\n",
    "    def create_test_message(self, query: str, prompt_template: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Create a test message structure\"\"\"\n",
    "        if prompt_template is None:\n",
    "            prompt_template = \"\"\"You are a helpful AI assistant. Based on the user query: $query\n",
    "\n",
    "Available functions:\n",
    "- search_products: Search for products in catalog\n",
    "- search_faqs: Search FAQ knowledge base\n",
    "- respond_to_user: Send final response to user\n",
    "\n",
    "Please help the user with their request.\"\"\"\n",
    "\n",
    "        return {\n",
    "            \"header\": {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"timestamp\": time.time()\n",
    "            },\n",
    "            \"payload\": {\n",
    "                \"agent\": {\n",
    "                    \"context\": {\n",
    "                        \"query\": query,\n",
    "                        \"history\": [],\n",
    "                        \"interactions\": []\n",
    "                    },\n",
    "                    \"prompt\": prompt_template\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def mock_function_response(self, function_name: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Mock function responses for testing\"\"\"\n",
    "        mock_responses = {\n",
    "            \"search_products\": {\n",
    "                \"products\": [\n",
    "                    {\"id\": 1, \"name\": \"Sample Product\", \"price\": \"$29.99\", \"color\": kwargs.get(\"color\", \"blue\")},\n",
    "                    {\"id\": 2, \"name\": \"Another Product\", \"price\": \"$39.99\", \"color\": kwargs.get(\"color\", \"red\")}\n",
    "                ],\n",
    "                \"total_results\": 2,\n",
    "                \"formatted_summary\": f\"Found 2 products matching '{kwargs.get('query', 'search term')}'\"\n",
    "            },\n",
    "            \"search_faqs\": {\n",
    "                \"faqs\": [\n",
    "                    {\n",
    "                        \"question\": \"How long does shipping take?\",\n",
    "                        \"answer\": \"Standard shipping takes 3-5 business days.\",\n",
    "                        \"category\": \"shipping\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"question\": \"What is your return policy?\",\n",
    "                        \"answer\": \"We accept returns within 30 days of purchase.\",\n",
    "                        \"category\": \"returns\"\n",
    "                    }\n",
    "                ],\n",
    "                \"total_results\": 2,\n",
    "                \"formatted_summary\": f\"Found 2 FAQs related to '{kwargs.get('text', 'question')}'\"\n",
    "            },\n",
    "            \"respond_to_user\": {\n",
    "                \"message_sent\": True,\n",
    "                \"content\": kwargs.get(\"content\", \"Response sent to user\"),\n",
    "                \"formatted_summary\": \"Response delivered to user successfully\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return mock_responses.get(function_name, {\"error\": f\"Unknown function: {function_name}\"})\n",
    "\n",
    "    def simulate_function_call(self, msg: Dict[str, Any], function_response: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate a function call completion\"\"\"\n",
    "        # Get the function call details\n",
    "        agent = msg.get(\"payload\", {}).get(\"agent\", {})\n",
    "        function_execution = agent.get(\"current_function_execution\", {})\n",
    "\n",
    "        # Create response message structure\n",
    "        response_msg = {\n",
    "            \"header\": msg.get(\"header\", {}),\n",
    "            \"payload\": {\n",
    "                \"agent\": {\n",
    "                    \"context\": agent.get(\"context\", {}),\n",
    "                    \"prompt\": agent.get(\"prompt\", \"\"),\n",
    "                    \"function_call\": {\n",
    "                        \"id\": function_execution.get(\"id\"),\n",
    "                        \"name\": function_execution.get(\"name\"),\n",
    "                        \"response\": function_response\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return response_msg\n",
    "\n",
    "    def process_query(self, query: str, prompt_template: str = None, auto_mock: bool = True):\n",
    "        \"\"\"Process a complete query with optional auto-mocking of function calls\"\"\"\n",
    "        print(f\"🔵 Processing query: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Step 1: Create initial message and process\n",
    "        msg = self.create_test_message(query, prompt_template)\n",
    "        messages = process_message(msg, self.llm_provider)\n",
    "\n",
    "        print(f\"📨 Generated {len(messages)} initial messages\")\n",
    "\n",
    "        current_msg = messages[0] if messages else None\n",
    "        step = 1\n",
    "\n",
    "        while current_msg:\n",
    "            print(f\"\\n🔄 Step {step}:\")\n",
    "\n",
    "            # Check if this message contains a function call\n",
    "            agent = current_msg.get(\"payload\", {}).get(\"agent\", {})\n",
    "            function_execution = agent.get(\"current_function_execution\")\n",
    "\n",
    "            if function_execution and \"name\" in function_execution:\n",
    "                func_name = function_execution[\"name\"]\n",
    "                func_args = function_execution.get(\"args\", {})\n",
    "                print(f\"🔧 Function call: {func_name}({func_args})\")\n",
    "\n",
    "                if auto_mock:\n",
    "                    # Auto-generate mock response\n",
    "                    mock_response = self.mock_function_response(func_name, **func_args)\n",
    "                    print(f\"🤖 Mock response: {mock_response}\")\n",
    "\n",
    "                    # Simulate function completion\n",
    "                    response_msg = self.simulate_function_call(current_msg, mock_response)\n",
    "                    current_msg = process_function_response(response_msg)\n",
    "\n",
    "                    # Check if we need to route to another function\n",
    "                    if current_msg.get(\"route_to_function\"):\n",
    "                        print(\"↪️  Routing to another function...\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"✅ Processing complete\")\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"⏸️  Manual mode - provide function response manually\")\n",
    "                    break\n",
    "            else:\n",
    "                # Direct response\n",
    "                response = agent.get(\"response\", \"\")\n",
    "                if response:\n",
    "                    print(f\"💬 Final response: {response}\")\n",
    "                else:\n",
    "                    print(\"⚠️  No response generated\")\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "            if step > 10:  # Safety break\n",
    "                print(\"⚠️  Too many steps, breaking...\")\n",
    "                break\n",
    "\n",
    "        return current_msg\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES AND HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def demo_basic_usage():\n",
    "    \"\"\"Demonstrate basic usage of the notebook LLM service\"\"\"\n",
    "    print(\"🚀 LLM Service Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize service\n",
    "    service = NotebookLLMService()\n",
    "\n",
    "    # Example 1: Product search query\n",
    "    print(\"\\n📦 Example 1: Product Search\")\n",
    "    result1 = service.process_query(\"I'm looking for a blue shirt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # Example 2: FAQ query\n",
    "    print(\"\\n❓ Example 2: FAQ Search\")\n",
    "    result2 = service.process_query(\"What is your return policy?\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # Example 3: Direct response\n",
    "    print(\"\\n💭 Example 3: General Question\")\n",
    "    result3 = service.process_query(\"What's the weather like?\")\n",
    "\n",
    "    return service\n",
    "\n",
    "def manual_function_testing():\n",
    "    \"\"\"Example of manual function testing\"\"\"\n",
    "    service = NotebookLLMService()\n",
    "\n",
    "    # Process with auto_mock=False for manual control\n",
    "    query = \"Find me red shoes\"\n",
    "    msg = service.create_test_message(query)\n",
    "    messages = process_message(msg, service.llm_provider)\n",
    "\n",
    "    print(\"Generated messages:\", len(messages))\n",
    "    print(\"First message structure:\", json.dumps(messages[0], indent=2, default=str))\n",
    "\n",
    "    return messages\n",
    "\n",
    "# Initialize global service for easy access\n",
    "llm_service = NotebookLLMService()\n",
    "\n",
    "print(\"🎉 Notebook LLM Service loaded successfully!\")\n",
    "print(\"📝 Usage:\")\n",
    "print(\"  - llm_service.process_query('your question here')\")\n",
    "print(\"  - demo_basic_usage()\")\n",
    "print(\"  - manual_function_testing()\")"
   ],
   "id": "b94e4a40ec8542c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
